---
title: "Práctica final Análisis de Datos: Ucinet"
author: "Castillo Millán, Sofía; Cirer Pastrana, Víctor Javier; Corporales Tur, Marta"
time:  "`Sys.Date()`"
output: 
  html_document: 
    keep_md: yes
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

# Cargamos librerías
library("tidyverse")
```

# Introducción

# Resumen

# Limpieza de datos

```{r path}
path = "data_basic_physionet/set-a/"# path training

lista_pacientes_set_a = dir(path) # lista  ficheros pacientes 

# length(lista_pacientes_set_a) # número pacientes en training
```


```{r cargarSetA}
# Lista path's a cada fichero de paciente
list_files = paste0(path, lista_pacientes_set_a)

# Función leer paciente
leer_paciente = function(file){
  read_csv(file, col_types = cols(Time = col_character(),
                                 Parameter = col_character(),
                                 Value = col_double())) %>%
    separate(Time, into = c("H","M"), sep = ":") %>% 
    mutate(Time_Minutes = as.numeric(H)*60 + as.numeric(M)) %>% 
    select(Time_Minutes, Parameter, Value)
}

# Leer_paciente(list_files[1])
raw_data = lapply(list_files,leer_paciente) # Lista de los datos por paciente

# Extraer perfiles "RecordID", "Age", "Gender", "Height", "Weight", "ICUType" 
perfil = function(data_paciente){
  data_paciente %>% 
    filter(Parameter %in% c("RecordID", "Age", "Gender", "Height", "ICUType", "Weight")) %>%
    select(-Time_Minutes) %>% distinct(Parameter, .keep_all = TRUE) %>% 
    spread(Parameter, Value)
}

# Guardamos los datos de perfil de cada paciente
perfiles = lapply(raw_data, perfil)%>%
  bind_rows() %>%
  select(RecordID, Age, Gender, Height, Weight, ICUType)

## Leer series
## Se modifica error de time
serie_UCI_parameter <- function(paciente,parameters){
  paciente %>%
    arrange(Parameter, Time_Minutes) %>%
    filter(Parameter %in% parameters) %>%
    add_column(RecordID = paciente[1,3]$Value) 
} 
```




```{r}
# Paso parámetros y apilo 
parameters = c("Albumin", "ALP", "ALT", "AST", "Bilirubin", "BUN", "Cholesterol", "Creatinine", "DiasABP", "FiO2", "GCS", "Glucose", "HCO3", "HCT", "HR", "K", "Lactate", "Mg", "MAP", "MechVent", "Na", "NIDiasABP", "NIMAP", "NISysABP", "PaCO2", "PaO2", "pH", "Platelets", "RespRate", "SaO2", "SysABP", "Temp", "TropI", "TropT", "Urine", "WBC")
series_parameters = lapply(raw_data, FUN = function(x) serie_UCI_parameter(x,parameters)) %>%
  bind_rows()
```


En resumen hasta ahora tenemos lo siguiente

```{r glimpse}
#set-a
glimpse(perfiles)
glimpse(series_parameters)
```


## Unificar: series, perfiles y scores

Nos faltan los scores clásicos que se utilizan eb las ICU. Estos ewstán el fichero Outcome-a.txt para el set-a


```{r}
scoresApath = "data_basic_physionet/Outcomes-a.txt"
scoresA = read_csv(scoresApath)
glimpse(scoresA)
Scores_perfilesA = inner_join(perfiles, scoresA, "RecordID")
glimpse(Scores_perfilesA)
```


### Extracción factores de las series 

Genero una tabla con resumenes de  las variables por paciente: media, desviación típica 

```{r}
series_summary = series_parameters %>%
  group_by(RecordID, Parameter) %>%
  summarise(count = n(), mean = mean(Value, na.rm = TRUE), sd = sd(Value, na.rm = TRUE))%>%
  gather(Stat, Value, count:sd) %>%
  ungroup() %>%
  transmute(RecordID, ParameterStat = paste0(Parameter, "_" ,Stat), Value) %>%
  spread(ParameterStat, Value)
```


```{r}
data_tidy=Scores_perfilesA %>%
  inner_join(series_summary)

head(data_tidy)
```


Para elegir las variables hemos dibujado todos los boxplots para saber cuáles eran más significativas teniendo en cuenta la variable `In-hospital_death`. Nos hemos quedado con las más significativas y en caso de duda entre dos hemos cogido la que no estaba en ninguno de los índices SOFA o SAPS. Como ejemplo, mostramos el boxplot del índice `GCS`.

```{r, echo = FALSE}

data_tidy$`In-hospital_death` = factor(data_tidy$`In-hospital_death`, labels = c("0","1"))

data_tidy %>%
  filter(!is.na(GCS_mean)) %>% 
  filter(!is.na(`In-hospital_death`)) %>%
  ggplot(data = data_tidy, mapping = aes(x = `In-hospital_death`, y = GCS_mean)) +
  geom_boxplot() + 
  ylim(6,7)
```


Nos quedamos con las siguientes 10 variables: 
 
1. GCS 
2. WBC  (NO - medias iguales)
3. RespRate (NO - medias iguales)
4. Albumin 
5. BUN (NO - medias iguales)
6. Cholesterol (NO - medis iguales)
7. PaO2
8. Glucose (NO - medias iguales)
9. HCO3 (SAPS)
10. Lactate (NO - medias iguales)

Una vez tenemos estas vamos a aplicar un `t.test` para saber si es significativa o no. Para ello necesitamos primero hacer un `var.test`, donde descubrimos si las varianzas son iguales o no para poderlo tener en cuenta en el `t.test`. Vamos a mostrar como hemos hecho una de las variables, el índice `GCS` del cual ya hemos visto el boxplot. Recordemos que si `In-hospital_death` es 0 el paciente vive y si es 1, muere en hospital.

Asignamos al valor `x` los valores de la variable a estudiar de los que sobreviven y al `y` los de los que mueren en el hospital, y aplicamos el `var.test` de las dos teniendo en cuenta que nuestra hipótesis nula HO es que las varianzas sean iguales y la hipótesis alternativa H1 es que sean distintas.
```{r}
x = data_tidy$GCS_mean[data_tidy$`In-hospital_death` == 0]
y = data_tidy$GCS_mean[data_tidy$`In-hospital_death` == 1]
var.test(x,y)
```

Una vez aplicaco el test de las varianzas nos fijamos en el valor del `p-valor`. Si este es menor que 0.05, rechazamos la hipótesis nula, por lo que podemos aplicar un `t.test` para varianzas distintas con H0 medias iguales y H1 medias distintas.
```{r}
t.test(x, y, var.equal = FALSE, alternative = "greater")
```

Como el `p-valor` es menor que 0.05, rechazamos la hipótesis nula, por lo que obtenemos que las medias son distintas. Como las medias son distintas para muerto o vivo, consideramos que la variable es significativa para el estudio y la seleccionamos. 

De las 10 variables previamente seleccionadas, las que nos hayan salido que tienen media distinta para supervivientes o muertos en la UCI son las que consideraremos para nuestro estudio final.


Por tanto, nos quedamos con las siguientes 4: (nos quedamos 4 en vez de 5, pues las otras nos han salido no significativas y así no estropeamos los datos que nos pueden dar más información)
1. GCS
2. Albumin
3. PaO2
4. HCO3 


```{r}
data_final <- data_tidy %>%
  select(RecordID, Age, Gender, Weight, ICUType, `SAPS-I`, SOFA, Length_of_stay, Survival, `In-hospital_death`, GCS_mean, Albumin_mean, PaO2_mean, HCO3_mean)
```


# Análisi Multivariado

Para realizar el análisis multivariado comenzaremos realizando un análisis de componentes principales para poder después continuar con un clustering adecuado a las componentes que veamos más representativas. Finalmente, acabaremos con un análisis discriminante para trabajar con variables cualitativas.

## Análisis de componentes principales

Comenzamos con una visualización de los datos, donde se comparan dos a dos cada una de las diferentes variables que hemos escogido para realizar este estudio.

```{r, echo=FALSE}
library("tidyverse")
library("FactoMineR")
library("factoextra")
library("relaimpo")
# Para utilizar ggpairs
library("GGally")

# Para utilizar multiplot
library("Rmisc")

# Para instalar ggbiplot (NO NECESARIO): instalar y cargar el paquete devtools, y ejecutar el comando
# install_github("vqv/ggbiplot")
# library("ggbiplot")

# Para utilizar ggpar
library("ggpubr")

nums = c(2:9,11:14) # quitamos el 1 que es el numero que indica el paciente y el 10 ('in-hospital_death') porque la hemos hecho no numérica para que fuera factor 0 o 1 para indicar "muerto" o no "muerto" y para las correlaciones da error
ggpairs(data_final[,nums],
        diag=list(
          continuous = wrap("barDiag",binwidth = 10, 
                            color = "black", fill= "blue")
        ),
        lower = list(
          continuous = wrap("points", size = 0.5, color = "black")
        ),
        
        title = "Correlaciones y distribución de las variables")->gg

gg

```
Como es muy complicado y laborioso tener en cuenta y determinar la relación que hay entre todas estas variables, vamos a describir sus
componentes principales, de manera que con un número menor de ellas podamos seguir trabajando sobre los datos perdiendo la menor información posible.

Comencemos calculando la matriz de covarianza. Teniendo en cuenta que algunas observaciones no tienen todas las variables completas (existen NA), procederemos a eliminar la variable obtenida en esa observación. Así obtenemos:

```{r}
covarianza <- cov(data_final[, nums], use = "na.or.complete")
covarianza
```

Observamos que la matriz es complicada de entender a simple vista, por ello utilizamos la función PCA, que nos devuelve los valores y vectores propios de esta matriz, dándonos información sobre el número de dimensiones que deberá tener nuestro análisis, es decir, el número de componentes principales.

```{r}
dataPCA <- PCA(data_final[, nums], graph = FALSE)
eig.val<-get_eigenvalue(dataPCA)
eig.val
```

Que podemos visualizar en el siguiente screeplot:


Realizamos ahora el 
```{r}
fviz_screeplot(dataPCA, addlabels = TRUE) -> screeplot
ggpar(screeplot, title = "Porcentaje de la varianza explicado por cada nueva variable",
xlab = "Dimensión",
ylab = "Porcentaje de varianza explicado")
```

Observamos que el análisis de componentes principales en este caso es bastante complicado ya que cada variable explica una parte importante de la observación. Aún así, podemos explicar una parte importante utilizando 2 variables en vez de 12 (un 32%). Se podrían haber utilizado más componentes, pero entonces no serviría de nada realizar este análisis ya que aumentaría mucho el contenido a estudiar y no sería significativo.

Veamos que variables representan cada componente:

```{r}
fviz_contrib(dataPCA, choice = "var", axes = 1) %>%
ggpar(title = "Contribución a Dim-1", ylab = "Contribución (%)") -> contrib1
fviz_contrib(dataPCA, choice = "var", axes = 2) %>%
ggpar(title = "A Dim-2", ylab = "Contribución (%)") -> contrib2
multiplot(contrib1, contrib2, cols = 2) 
```

Observamos que para cada dimensión tenemos las variables:

1ª Dimensión: SOFA, SAPS y GCS_mean.
2ª Dimensión: Age y ICUType.

Ahora nos disponemos a analizar las dos componentes para compararlos mediante un biplot.

```{r}
as_tibble(dataPCA$ind$coord, rownames = NA) %>% rownames_to_column() %>%
filter(Dim.1 > 3 | Dim.2 < -2.5) -> addToBiplot12 #Etiquetamos a los outliers
#Representamos un biplot de las componentes 1 y 2
fviz_pca_biplot(dataPCA,axes=c(1,2), repel = TRUE,geom = "point",
addEllipses = TRUE,col.var = 'darkred', invisible = "quanti.sup") %>%
ggpar(xlim=c(-4,7),ylim=c(-5,4),title = "Biplot - Dim 1 y Dim 2",
xlab = "Dim 1 (20%)",ylab = "Dim 2 (11%)") +
geom_text(data = addToBiplot12, mapping = aes(x = Dim.1, y = Dim.2, label = rowname))
``` 

Hemos podido ver que debido a la poca cantidad de información que representan las componentes principales, ya que no existen variables dominantes frente a las demás (las variables dan prácticamente la misma información), no hemos podido realizar un ACP satisfactorio.

Observamos que en este caso no podemos llegar a una conclusión ya que los puntos están prácticamente uniformemente distribuidos dentro del biplot. Podemos ver cierta tendencia hacia la derecha del biplot que puede ayudar a entender que la tasa de muerte de una persona está mínimamente representada por la dimensión 1, mientras que la dimensión 2 no dice nada.

De forma acertada han aparecido las variables SOFA y SAPS-I en la primera componente principal ya que, como ya sabemos, suponen un buen coeficiente determinante de la condición del paciente ya que engloba todas las demás variables.

```
Comprobación del ACP: 
```{r}
library("ggfortify")

autoplot(data_final.acp,
         data = data_final,
         colour = 'ICUType',
         loadings = TRUE,
         loadings.colour = 'blue',
         loadings.label = TRUE,
         loadings.label.size = 3
         )
```
```

## Escalamiento multidimensional

Continuaremos con el escalamiento multidimensional. Comenzaremos calculando la matriz de distancias utilizando la distancia de Gower. Esta matriz es muy grande (más de 15 millones de entradas), por tanto omitiremos enseñarla. Después, mediante la función fit, haremos un ACP de dos componentes (igual que el realizado en el apartado anterior), para poder agrupar las variables y analizarlas más fácilmente. 

````{r}
library("cluster")
dist=as.matrix(daisy(data_final[, nums], metric="gower"))
fit <- cmdscale(dist,eig=TRUE, k=2)
```
Los metemos en una tibble con etiqueta dependiendo de la UCI.
```{r}
datos <- tibble(uci = as.factor(data_final[, nums]$ICUType), ihd=data_final[['In-hospital_death']], comp1 = fit$points[,1], comp2 = fit$points[,2])

```
A continuación dibujamos las muestras de pacientes coloreándolas en función del tipo de UCI:
```{r}
datos %>% ggplot(aes(x=comp1, y=comp2)) +
  geom_point(aes(color=uci))
```
Podemos ver que la componente 2 parece influir en el tipo de UCI. Una componente 2 baja suele tener pacientes que son atendidos en la UCI 1, subiendo un poco esta componente, la UCI 2, con un poco más nos encontramos pacientes de la UCI 3 y, con la componente dos más alta, con los pacientes de la UCI 4.

Ahora vamos a estudiar como afectan estas componentes a las muertes en el hospital. En el siguiente gráfico, pintamos de color verde los muertos y de color rojo los vivos. Además, mantenemos como etiqueta el tipo de UCI.

```{r}
datos %>% ggplot(aes(x=comp1, y=comp2)) +
  geom_label(aes(label=uci, color=ihd))
```
A pesar de que los datos son bastante homogéneos, podemos apreciar una ligera propensión a que las muertes en el hospital se produzcan en pacientes cuya componente 1 es más baja, no obstante, también mueren pacientes con componente 1 alta. 

La homogeneidad de esta muestra nos hace pensar que es una mala idea hacer clustering, aún así, como hemos visto bastante diferenciación dependiendo de las UCIs, probamos a hacer un algoritmo k-means con 4 clusters (por el número de UCIs).


```{r}
kmeans_result <- datos %>% dplyr::select(comp1,comp2) %>% kmeans(4)
datos %>% mutate(group = as.factor(kmeans_result$cluster)) %>%
  ggplot(aes(x=comp1, y=comp2)) +
  geom_point(aes(color=group))
  


```
Como habíamos esperado, por como es la muestra, el clustering no es significativo ya que hace la separación, prácticamente, en los ejes de coordenadas.

## Análisis discriminante

```{r}

```




